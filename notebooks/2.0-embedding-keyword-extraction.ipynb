{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383641e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "\n",
    "# Import our custom modules\n",
    "from src.config import config, PROJECT_ROOT\n",
    "from src.features.build_features import get_embedding_model, find_representative_ngrams\n",
    "\n",
    "# Initialize tqdm for pandas operations (like .progress_apply)\n",
    "tqdm.pandas(desc=\"Processing Documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8a94541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full dataset from: C:\\Users\\lib9\\weak-signals-new\\data\\processed\\cv_arxiv_data_2010-2022.parquet\n",
      "Successfully loaded 8269 total records.\n",
      "\n",
      "Filtering for year 2010...\n",
      "Found 25 records for the year 2010.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 (Corrected)\n",
    "\n",
    "# 1. Load the full processed dataset\n",
    "processed_data_path = PROJECT_ROOT / config.data.processed_path\n",
    "print(f\"Loading full dataset from: {processed_data_path}\")\n",
    "full_df = pd.read_parquet(processed_data_path)\n",
    "print(f\"Successfully loaded {len(full_df)} total records.\")\n",
    "\n",
    "# 2. Filter for the year 2010\n",
    "print(\"\\nFiltering for year 2010...\")\n",
    "# The 'published' column is already an integer year, so we can filter directly\n",
    "df = full_df[full_df['published'] == 2010].copy()\n",
    "print(f\"Found {len(df)} records for the year 2010.\")\n",
    "\n",
    "# 3. Load the sentence-transformer model\n",
    "# This step is unchanged\n",
    "embedding_model = get_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d435f015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing 25 documents for year 2010 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b31fba6c5e14ba2a78c06bf432b489b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Documents for 2010:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted top 249 keywords for 2010.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize tqdm for pandas operations (like .progress_apply)\n",
    "tqdm.pandas(desc=\"Processing Documents for 2010\")\n",
    "\n",
    "print(f\"--- Processing {len(df)} documents for year 2010 ---\")\n",
    "\n",
    "# Use .progress_apply to show a progress bar for this long step\n",
    "list_of_keyword_lists = df['summary'].progress_apply(\n",
    "    lambda doc: find_representative_ngrams(doc, embedding_model)\n",
    ")\n",
    "\n",
    "# Flatten the list of lists into a single list of all keywords for the year\n",
    "all_keywords_for_2010 = list(itertools.chain.from_iterable(list_of_keyword_lists))\n",
    "\n",
    "# Count the frequency of each keyword and get the top 300\n",
    "keyword_counts = Counter(all_keywords_for_2010)\n",
    "top_300_keywords = [kw for kw, count in keyword_counts.most_common(300)]\n",
    "\n",
    "print(f\"\\nExtracted top {len(top_300_keywords)} keywords for 2010.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0179a012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique keywords before cleaning: 249\n",
      "Total keywords for 2010 after cleaning: 249\n",
      "\n",
      "Sample of final, cleaned keywords for 2010:\n",
      "['3d model natural', 'accurately detecting pedestrians', 'alignment point sets', 'alternative online boosting', 'approach point detection', 'approximate spectral analysis', 'asymmetric boosting', 'asymmetric boosting methods', 'automatic video segmentation', 'boosting', 'boosting algorithm', 'boosting algorithms', 'boosting algorithms improve', 'boosting detectors', 'boosting like adaboost', 'bridge computer vision', 'bright mag stars', 'caenorhabditis elegans', 'caenorhabditis elegans elegans', 'calibration multi camera', 'camera positions orientations', 'cameras global coordinate', 'cascade boosting', 'cascade boosting framework', 'catalog real stars']\n"
     ]
    }
   ],
   "source": [
    "# Import the standard list of English stop words\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# 1. Use a set to get unique keywords from the top 300 list\n",
    "unique_keywords = set(top_300_keywords)\n",
    "print(f\"Total unique keywords before cleaning: {len(unique_keywords)}\")\n",
    "\n",
    "# 2. Filter out common English stop words, single-character words, and numeric-only words\n",
    "final_keywords_2010 = [\n",
    "    keyword for keyword in unique_keywords \n",
    "    if keyword not in ENGLISH_STOP_WORDS and len(keyword) > 1 and not keyword.isdigit()\n",
    "]\n",
    "\n",
    "# 3. Sort the final list alphabetically for consistent output\n",
    "final_keywords_2010.sort()\n",
    "\n",
    "print(f\"Total keywords for 2010 after cleaning: {len(final_keywords_2010)}\")\n",
    "print(\"\\nSample of final, cleaned keywords for 2010:\")\n",
    "print(final_keywords_2010[:25])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
